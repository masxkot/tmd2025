{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **GAN を用いた SMILES 文字列生成のコードまとめ**\n",
        "\n",
        "### **1. 概要**\n",
        "このコードは、**GAN（Generative Adversarial Network）を用いて SMILES 文字列（化学構造表記）を生成** するものです。  \n",
        "RDKit を利用し、分子構造の妥当性を考慮しながら SMILES を生成します。  \n",
        "判別器 (Discriminator) `D` と 生成器 (Generator) `G` の対抗学習により、より本物らしい SMILES を生成することを目指します。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. コードの構成**\n",
        "### **① データの準備**\n",
        "```python\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "directory = '/content/drive/My Drive/day6'\n",
        "property_df = pd.read_csv(os.path.join(directory, 'property_df.csv'))\n",
        "```\n",
        "- Google Drive からデータ（`property_df.csv`）を読み込み。\n",
        "- `property_df` には、**SMILES 文字列**が含まれる。\n",
        "\n",
        "---\n",
        "\n",
        "### **② SMILES データの前処理**\n",
        "```python\n",
        "SMILES_COL = \"Open Babel SMILES\"\n",
        "SMILES_MAXLEN = 128\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 256\n",
        "N_HIDDEN = 100\n",
        "N_LATENT = 100\n",
        "\n",
        "vocab_freq =  {}\n",
        "word_length_dist = []\n",
        "for smile in property_df[SMILES_COL]:\n",
        "    for s in smile:\n",
        "        if s not in vocab_freq.keys():\n",
        "            vocab_freq[s] = 0\n",
        "        vocab_freq[s] += 1\n",
        "    word_length_dist.append(len(smile))\n",
        "\n",
        "vocab = list(vocab_freq.keys())\n",
        "N_INPUT = len(vocab) * SMILES_MAXLEN\n",
        "```\n",
        "- **SMILES の最大長 (`SMILES_MAXLEN = 128`) を設定**。\n",
        "- **SMILES 文字の vocab（語彙）を作成し、N_INPUT を計算**。\n",
        "\n",
        "---\n",
        "\n",
        "### **③ GAN の構築**\n",
        "```python\n",
        "G = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_LATENT, N_HIDDEN),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_HIDDEN, N_INPUT),\n",
        "    torch.nn.Tanh()\n",
        ").to(device)\n",
        "\n",
        "D = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_INPUT, N_HIDDEN),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(N_HIDDEN, 1),\n",
        "    torch.nn.Sigmoid()\n",
        ").to(device)\n",
        "```\n",
        "- **生成器 (G)**\n",
        "  - ランダムノイズ `z (N_LATENT)` を入力し、SMILES 文字の one-hot 表現を出力。\n",
        "- **判別器 (D)**\n",
        "  - 入力された SMILES が本物かどうかを判定（出力は 0～1）。\n",
        "\n",
        "---\n",
        "\n",
        "### **④ One-hot エンコーディングとデータローダーの作成**\n",
        "```python\n",
        "def smile2vec(vocab, vecsize, smile):\n",
        "    vec = []\n",
        "    for i in range(vecsize):\n",
        "        v = [0 for _ in range(len(vocab))]\n",
        "        if i < len(smile):\n",
        "            v[vocab.index(smile[i])] = 1\n",
        "        vec += v\n",
        "    return vec\n",
        "\n",
        "X = np.array([smile2vec(vocab, SMILES_MAXLEN, smile) for smile in list(property_df[SMILES_COL])])\n",
        "X_tensor = torch.from_numpy(X).float()\n",
        "dataset = TensorDataset(X_tensor)\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "```\n",
        "- **SMILES を One-hot ベクトルに変換** (`smile2vec`)\n",
        "- **PyTorch の `DataLoader` を作成**。\n",
        "\n",
        "---\n",
        "\n",
        "### **⑤ RDKit を用いた SMILES の修正**\n",
        "```python\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "def get_best_smile(out_tensor):\n",
        "    best_smile = \"\"\n",
        "    for vec in out_tensor:\n",
        "        vec = vec.reshape(SMILES_MAXLEN, len(vocab))\n",
        "        smile = \"\".join([vocab[torch.argmax(v).item()] for v in vec])\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "        while not mol:\n",
        "            if len(smile) == 0: break\n",
        "            smile = smile[:-1]\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "        if len(best_smile) < len(smile):\n",
        "            best_smile = smile\n",
        "\n",
        "    return best_smile\n",
        "```\n",
        "- **生成されたベクトルを SMILES に変換**。\n",
        "- **RDKit を使って無効な SMILES を修正**。\n",
        "\n",
        "---\n",
        "\n",
        "### **⑥ GAN の学習**\n",
        "```python\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data in enumerate(data_loader):\n",
        "\n",
        "        # 判別器の学習\n",
        "        outputs = D(data[0].to(device))\n",
        "        real_labels = torch.ones(outputs.shape[0], 1).to(device)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(BATCH_SIZE, N_LATENT).to(device)\n",
        "        fake_data = G(z)\n",
        "        outputs = D(fake_data)\n",
        "        fake_labels = torch.zeros(outputs.shape[0], 1).to(device)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        if (i % 2) == 0:\n",
        "            d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # 生成器の学習\n",
        "        z = torch.randn(BATCH_SIZE, N_LATENT).to(device)\n",
        "        fake_data = G(z)\n",
        "        outputs = D(fake_data)\n",
        "\n",
        "        real_labels = torch.ones(outputs.shape[0], 1).to(device)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # ログの出力\n",
        "        if (i+1) % 10 == 0 and (epoch+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.4f}, D(G(z)): {:.4f}'\n",
        "                  .format(epoch, NUM_EPOCHS, i+1, total_step, d_loss.item(), g_loss.item(),\n",
        "                          real_score.mean().item(), fake_score.mean().item()))\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Epoch[{}/{}], Generated SMILES: {}\".format(\n",
        "            epoch+1, NUM_EPOCHS, get_best_smile(fake_data)))\n",
        "```\n",
        "- **判別器 `D` の学習**\n",
        "  - 本物データを 1 に、偽物データを 0 に判定できるように学習。\n",
        "- **生成器 `G` の学習**\n",
        "  - 偽物データを 1 に判定させるように学習。\n",
        "- **RDKit を用いて生成 SMILES をチェック**。\n",
        "\n",
        "---\n",
        "\n",
        "### **7. まとめ**\n",
        "- **SMILES 文字列を One-hot ベクトル化** し、GAN で学習。\n",
        "- **判別器 `D` は本物・偽物を判別** し、**生成器 `G` は偽物をより本物らしく改善**。\n",
        "- **RDKit を利用し、無効な SMILES を修正**。\n",
        "- **学習が進むと、本物らしい SMILES が生成される**。\n",
        "\n",
        "このコードを改善することで、より高品質な化学分子を生成できる可能性があります。"
      ],
      "metadata": {
        "id": "CR7XzRWhrtxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpR1_9x_1woW",
        "outputId": "79d25b73-b1f3-40ed-b52f-27e82d15ca87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、PyTorchを使用して計算デバイス（CPUまたはGPU）を自動的に選択する処理を行っています。\n",
        "\n",
        "---\n",
        "\n",
        "### **コードの詳細解説**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "```\n",
        "- PyTorchのライブラリをインポートします。\n",
        "\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "```\n",
        "- `torch.cuda.is_available()` は、CUDA対応のGPUが使用可能かどうかを確認する関数です。\n",
        "  - **True** の場合：GPU（CUDA）を利用できるので、`'cuda'` を選択\n",
        "  - **False** の場合：GPUを利用できないので、`'cpu'` を選択\n",
        "- `torch.device(...)` は、PyTorchのテンソルをどのデバイス（CPU/GPU）に配置するかを指定するオブジェクトを作成します。\n",
        "\n",
        "```python\n",
        "device\n",
        "```\n",
        "- `device` の値を表示します。\n",
        "- 例えば、CUDA対応GPUが使える場合は `device('cuda')`、使えない場合は `device('cpu')` になります。\n",
        "\n",
        "---\n",
        "\n",
        "### **動作例**\n",
        "1. GPUが利用可能な場合:\n",
        "   ```python\n",
        "   torch.cuda.is_available()  # → True\n",
        "   print(device)  # → cuda\n",
        "   ```\n",
        "2. GPUが利用不可の場合:\n",
        "   ```python\n",
        "   torch.cuda.is_available()  # → False\n",
        "   print(device)  # → cpu\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **実際の利用例**\n",
        "この `device` を使って、テンソルやモデルを適切なデバイスに配置できます。\n",
        "\n",
        "```python\n",
        "tensor = torch.tensor([1, 2, 3], device=device)  # 指定したデバイスにテンソルを配置\n",
        "model = MyModel().to(device)  # PyTorchモデルを指定デバイスに転送\n",
        "```\n",
        "\n",
        "このコードにより、GPUが使える環境では自動的にGPUを使い、使えない場合はCPUで計算が行われるようになります。"
      ],
      "metadata": {
        "id": "4vVp5Nih7sXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "directory = '/content/drive/My Drive/day6'\n",
        "property_df = pd.read_csv(os.path.join(directory, 'property_df.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH6nAK5L14Fe",
        "outputId": "4aa609d2-dd95-458b-dfbb-355d73000d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、Google Colabを使用してGoogle Driveに保存されたCSVファイルを読み込む処理を行っています。\n",
        "\n",
        "---\n",
        "\n",
        "### **コードの詳細解説**\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "```\n",
        "- `google.colab` の `drive` モジュールをインポートし、Google DriveをColabにマウントするための準備をします。\n",
        "- `pandas` をインポートしてデータ解析を行えるようにします。\n",
        "- `os` をインポートして、ファイルパスの操作を簡単にします。\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "- Google Driveを `/content/drive` にマウントします。\n",
        "- 実行すると、認証のためのリンクが表示され、Googleアカウントで許可を与えると、Google Drive内のファイルにアクセスできるようになります。\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "directory = '/content/drive/My Drive/day6'\n",
        "```\n",
        "- Google Drive内のフォルダ `/My Drive/day6` を指定し、このディレクトリ内のファイルを扱う準備をします。\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "property_df = pd.read_csv(os.path.join(directory, 'property_df.csv'))\n",
        "```\n",
        "- `os.path.join(directory, 'property_df.csv')` により、フルパス `/content/drive/My Drive/day6/property_df.csv` を作成します。\n",
        "- `pd.read_csv()` を使って、指定したCSVファイルをDataFrame (`property_df`) として読み込みます。\n",
        "\n",
        "---\n",
        "\n",
        "### **動作の流れ**\n",
        "1. Google DriveをColabにマウント。\n",
        "2. Google Driveの特定フォルダ (`day6`) を指定。\n",
        "3. そのフォルダ内の `property_df.csv` を読み込み、データフレームに格納。\n",
        "\n",
        "---\n",
        "\n",
        "### **注意点**\n",
        "- マウント後にColabのセッションがリセットされると、再マウントが必要になります。\n",
        "- `property_df.csv` のパスが正しいか確認し、フォルダ `day6` の中に正しく配置されているかチェックしてください。\n",
        "- `My Drive` の表記は環境によって異なる可能性があるため、`drive.mount('/content/drive')` 後に `!ls /content/drive/My\\ Drive` で確認するとよいです。\n",
        "\n",
        "このコードは、Google Drive上のデータをColabで分析する際に役立ちます！"
      ],
      "metadata": {
        "id": "xhjDWvJJ7210"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMILES_COL = \"Open Babel SMILES\"\n",
        "SMILES_MAXLEN = 128\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 256\n",
        "N_HIDDEN = 100\n",
        "N_LATENT = 100\n",
        "\n",
        "vocab_freq =  {}\n",
        "word_length_dist = []\n",
        "for smile in property_df[SMILES_COL]:\n",
        "    for s in smile:\n",
        "        if s not in vocab_freq.keys():\n",
        "            vocab_freq[s] = 0\n",
        "        vocab_freq[s] += 1\n",
        "    word_length_dist.append(len(smile))\n",
        "\n",
        "vocab = list(vocab_freq.keys())\n",
        "N_INPUT = len(vocab)*SMILES_MAXLEN\n"
      ],
      "metadata": {
        "id": "sXjKSt5L1-HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、SMILES（Simplified Molecular Input Line Entry System）表記を用いた化学データを扱うための前処理を行い、ニューラルネットワークのハイパーパラメータを設定する処理を含んでいます。  \n",
        "\n",
        "---\n",
        "\n",
        "## **コードの詳細解説**\n",
        "\n",
        "### **1. 定数の設定**\n",
        "```python\n",
        "SMILES_COL = \"Open Babel SMILES\"\n",
        "SMILES_MAXLEN = 128\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 256\n",
        "N_HIDDEN = 100\n",
        "N_LATENT = 100\n",
        "```\n",
        "- `SMILES_COL`：DataFrame (`property_df`) 内でSMILES表記の分子構造が格納されているカラム名。\n",
        "- `SMILES_MAXLEN`：SMILES文字列の最大長。最大128文字までの分子表記を考慮。\n",
        "- `LEARNING_RATE`：学習率（`1e-6 = 0.000001`）、ニューラルネットワークの学習のステップサイズ。\n",
        "- `NUM_EPOCHS`：学習を繰り返す回数（エポック数）。\n",
        "- `BATCH_SIZE`：ミニバッチ学習時のデータサイズ（256個ずつ学習）。\n",
        "- `N_HIDDEN`：隠れ層のニューロン数（100）。\n",
        "- `N_LATENT`：潜在変数（latent vector）の次元数（100）。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. SMILESデータの前処理**\n",
        "```python\n",
        "vocab_freq =  {}\n",
        "word_length_dist = []\n",
        "```\n",
        "- `vocab_freq`：SMILES文字列に含まれる各文字の出現回数を記録する辞書。\n",
        "- `word_length_dist`：各SMILES文字列の長さを格納するリスト。\n",
        "\n",
        "```python\n",
        "for smile in property_df[SMILES_COL]:\n",
        "    for s in smile:\n",
        "        if s not in vocab_freq.keys():\n",
        "            vocab_freq[s] = 0\n",
        "        vocab_freq[s] += 1\n",
        "    word_length_dist.append(len(smile))\n",
        "```\n",
        "このループでは、以下の処理を行います：\n",
        "1. `property_df[SMILES_COL]` から各SMILES文字列を取得。\n",
        "2. 文字列内の各文字 (`s`) を確認し、`vocab_freq` に登録。\n",
        "   - もし辞書に存在しなければ初期値 `0` を設定。\n",
        "   - 出現回数をカウント (`+= 1`)。\n",
        "3. 各SMILESの長さを `word_length_dist` に保存。\n",
        "\n",
        "この処理が完了すると：\n",
        "- `vocab_freq` は、SMILESに含まれる文字（原子や結合の表現）ごとの出現頻度を保持。\n",
        "- `word_length_dist` には、すべてのSMILESの長さがリストとして格納される。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. 語彙（vocab）の作成**\n",
        "```python\n",
        "vocab = list(vocab_freq.keys())\n",
        "```\n",
        "- `vocab` は、SMILES文字列に含まれる一意な文字のリスト。\n",
        "- 例えば、SMILESに `C`, `O`, `N`, `=`, `#`, `[`, `]` などの文字が含まれていた場合、`vocab` は `['C', 'O', 'N', '=', '#', '[', ']']` というリストになる。\n",
        "\n",
        "```python\n",
        "N_INPUT = len(vocab) * SMILES_MAXLEN\n",
        "```\n",
        "- `N_INPUT` は、ニューラルネットワークの入力次元数を計算。\n",
        "  - `len(vocab)` は、SMILESに登場するユニークな文字の数（語彙サイズ）。\n",
        "  - `SMILES_MAXLEN` は、SMILESの最大長。\n",
        "  - 例えば、`len(vocab) = 50` で `SMILES_MAXLEN = 128` の場合、`N_INPUT = 50 × 128 = 6400` となる。\n",
        "\n",
        "---\n",
        "\n",
        "## **まとめ**\n",
        "- **定数の設定**：ニューラルネットワークのハイパーパラメータを定義。\n",
        "- **SMILESデータの処理**：\n",
        "  - 各SMILES文字列のユニークな文字（語彙）を取得。\n",
        "  - 各文字の出現回数をカウント。\n",
        "  - 各SMILESの長さを記録。\n",
        "- **入力次元の計算**：\n",
        "  - 語彙サイズとSMILESの最大長を用いて、ニューラルネットワークの入力サイズ (`N_INPUT`) を決定。\n",
        "\n",
        "この前処理は、SMILESデータを機械学習モデルに入力できる形（例えば、one-hotベクトルや埋め込みベクトル）に変換するための準備として重要です。"
      ],
      "metadata": {
        "id": "HwV90S8I8FB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_LATENT, N_HIDDEN),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_HIDDEN, N_INPUT),\n",
        "    torch.nn.Tanh()).to(device)\n",
        "\n",
        "D = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_INPUT, N_HIDDEN),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(N_HIDDEN, 1),\n",
        "    torch.nn.Sigmoid()).to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=LEARNING_RATE)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "ZUeXKvR-2FTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、**Generative Adversarial Network (GAN)** のジェネレーター (G) とディスクリミネーター (D) を定義し、それらを学習するための損失関数と最適化手法を設定しています。\n",
        "\n",
        "---\n",
        "\n",
        "## **1. ジェネレーター (G) の定義**\n",
        "```python\n",
        "G = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_LATENT, N_HIDDEN),  # 入力層（潜在ベクトル → 隠れ層）\n",
        "    torch.nn.ReLU(),                      # 活性化関数（ReLU）\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),  # 隠れ層（中間層）\n",
        "    torch.nn.ReLU(),                      # 活性化関数（ReLU）\n",
        "    torch.nn.Linear(N_HIDDEN, N_INPUT),   # 出力層（隠れ層 → 出力層）\n",
        "    torch.nn.Tanh()                       # 活性化関数（Tanh）\n",
        ").to(device)\n",
        "```\n",
        "### **ジェネレーターの構造**\n",
        "- `N_LATENT`（潜在ベクトル）を入力し、`N_INPUT`（SMILESデータの1次元表現）を出力。\n",
        "- **3層の線形変換 (Linear) を持つネットワーク**\n",
        "  1. **(N_LATENT → N_HIDDEN)**\n",
        "  2. **(N_HIDDEN → N_HIDDEN)**\n",
        "  3. **(N_HIDDEN → N_INPUT)**\n",
        "- 活性化関数：\n",
        "  - **ReLU**：隠れ層で使用（勾配消失問題を防ぐため）\n",
        "  - **Tanh**：出力層で使用（GANでは出力を -1～1 の範囲にするため一般的）\n",
        "\n",
        "### **動作イメージ**\n",
        "- `G` はランダムなノイズ (`N_LATENT` 次元) を受け取り、 `N_INPUT` 次元のデータ（生成したSMILESの数値表現）を出力する。\n",
        "- **本物っぽいデータを生成する役割** を持つ。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. ディスクリミネーター (D) の定義**\n",
        "```python\n",
        "D = torch.nn.Sequential(\n",
        "    torch.nn.Linear(N_INPUT, N_HIDDEN),   # 入力層（SMILESデータ → 隠れ層）\n",
        "    torch.nn.LeakyReLU(0.2),              # 活性化関数（LeakyReLU）\n",
        "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),  # 隠れ層\n",
        "    torch.nn.LeakyReLU(0.2),              # 活性化関数（LeakyReLU）\n",
        "    torch.nn.Linear(N_HIDDEN, 1),         # 出力層（1つのスカラ値: 本物 or 偽）\n",
        "    torch.nn.Sigmoid()                    # 活性化関数（Sigmoidで確率を出力）\n",
        ").to(device)\n",
        "```\n",
        "### **ディスクリミネーターの構造**\n",
        "- **3層の線形変換 (Linear) を持つネットワーク**\n",
        "  1. **(N_INPUT → N_HIDDEN)**\n",
        "  2. **(N_HIDDEN → N_HIDDEN)**\n",
        "  3. **(N_HIDDEN → 1)**\n",
        "- 活性化関数：\n",
        "  - **LeakyReLU(0.2)**：負の値も少し通すことで学習の停滞を防ぐ（`α=0.2`）。\n",
        "  - **Sigmoid**：最後の出力を `[0,1]` の確率に変換（`1 = 本物, 0 = 偽`）。\n",
        "\n",
        "### **動作イメージ**\n",
        "- `D` は `N_INPUT`（SMILESデータの数値表現）を受け取り、それが本物 (`1`) か偽物 (`0`) かを確率で出力。\n",
        "- **本物と偽物を見分ける役割** を持つ。\n",
        "\n",
        "---\n",
        "\n",
        "## **3. 損失関数の定義**\n",
        "```python\n",
        "criterion = torch.nn.BCELoss()\n",
        "```\n",
        "- **BCE (Binary Cross Entropy) 損失関数**：\n",
        "  - ディスクリミネーター `D` の出力（確率）と、ラベル（本物: `1`, 偽物: `0`）の誤差を測る。\n",
        "  - GANで一般的に使用される損失関数。\n",
        "\n",
        "---\n",
        "\n",
        "## **4. 最適化手法の定義**\n",
        "```python\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=LEARNING_RATE)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=LEARNING_RATE)\n",
        "```\n",
        "- `torch.optim.Adam`：適応学習率を持つ Adam 最適化アルゴリズムを使用。\n",
        "- `LEARNING_RATE`（学習率）を指定。\n",
        "- `D.parameters()`：ディスクリミネーターのパラメータを更新。\n",
        "- `G.parameters()`：ジェネレーターのパラメータを更新。\n",
        "\n",
        "---\n",
        "\n",
        "## **GAN の学習の流れ**\n",
        "1. **ディスクリミネーターの学習**\n",
        "   - `G` が生成した偽データと、本物のデータを `D` に入力。\n",
        "   - `D` が正しく分類できるように学習。\n",
        "   - **本物には 1、偽物には 0 を出力するように損失関数 BCE で学習**。\n",
        "\n",
        "2. **ジェネレーターの学習**\n",
        "   - `G` の出力（偽データ）を `D` に入力し、`D(G(z))` の出力を `1` に近づけるように学習。\n",
        "   - **偽データを本物に近づけるように `G` を改善**。\n",
        "\n",
        "---\n",
        "\n",
        "## **まとめ**\n",
        "- **`G` (ジェネレーター)**：\n",
        "  - ノイズ (`N_LATENT`) を入力し、本物っぽい `N_INPUT` のデータを生成するネットワーク。\n",
        "  - ReLU（隠れ層）＋Tanh（出力層）を使用。\n",
        "\n",
        "- **`D` (ディスクリミネーター)**：\n",
        "  - 入力 (`N_INPUT`) を受け取り、それが本物か偽物かを判別するネットワーク。\n",
        "  - LeakyReLU（隠れ層）＋Sigmoid（出力層）を使用。\n",
        "\n",
        "- **損失関数**：\n",
        "  - `BCE (Binary Cross Entropy) Loss` を使用。\n",
        "\n",
        "- **最適化手法**：\n",
        "  - `Adam` を使用し、ジェネレーターとディスクリミネーターをそれぞれ更新。\n",
        "\n",
        "このコードは、**SMILES表記の分子データを生成するための GAN** のベースモデルを作成するものです！"
      ],
      "metadata": {
        "id": "cwFLk-118UnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def smile2vec(vocab, vecsize, smile):\n",
        "    vec = []\n",
        "    for i in range(vecsize):\n",
        "        v = [0 for _ in range(len(vocab))]\n",
        "        if i < len(smile):\n",
        "            v[vocab.index(smile[i])] = 1\n",
        "        vec += v\n",
        "    return vec\n",
        "\n",
        "\n",
        "X = np.array([smile2vec(vocab, SMILES_MAXLEN, smile) for smile in list(property_df[SMILES_COL])])\n",
        "X_tensor = torch.from_numpy(X).float()\n",
        "dataset = TensorDataset(X_tensor)\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "UP3HiWbO2fxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、**SMILES表記の化学分子データを数値ベクトルに変換し、それをPyTorchのデータローダー（DataLoader）で扱える形にする処理** を行っています。  \n",
        "\n",
        "---\n",
        "\n",
        "## **1. 必要なライブラリのインポート**\n",
        "```python\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "```\n",
        "- `numpy`：データの数値計算（行列操作）に使用。\n",
        "- `torch`：PyTorchのテンソル操作に使用。\n",
        "- `torch.utils.data.TensorDataset`：PyTorchのデータセットクラス。\n",
        "- `torch.utils.data.DataLoader`：データセットをバッチ単位で扱えるようにするためのデータローダー。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. SMILES文字列をベクトル化する関数**\n",
        "```python\n",
        "def smile2vec(vocab, vecsize, smile):\n",
        "    vec = []  # ベクトルを格納するリスト\n",
        "    for i in range(vecsize):\n",
        "        v = [0 for _ in range(len(vocab))]  # 語彙サイズ分のゼロベクトルを作成\n",
        "        if i < len(smile):  # SMILES文字列の範囲内であれば\n",
        "            v[vocab.index(smile[i])] = 1  # one-hotエンコーディング\n",
        "        vec += v  # ベクトルを連結\n",
        "    return vec\n",
        "```\n",
        "### **関数の詳細**\n",
        "- `vocab`: SMILES表記に含まれるユニークな文字のリスト（語彙）。\n",
        "- `vecsize`: 最大SMILES長 (`SMILES_MAXLEN`)。\n",
        "- `smile`: 変換するSMILES文字列。\n",
        "\n",
        "**処理の流れ**\n",
        "1. `vecsize`（= `SMILES_MAXLEN`）分のループを回す。\n",
        "2. `vocab` のサイズだけ `0` のリスト（one-hotベクトル）を作る。\n",
        "3. `smile` の `i` 番目の文字が `vocab` にある場合、その位置を `1` にする（one-hotベクトル）。\n",
        "4. すべてのone-hotベクトルを連結して1つの長いベクトルにする。\n",
        "\n",
        "### **例**\n",
        "#### **入力**\n",
        "```python\n",
        "vocab = ['C', 'O', 'N']\n",
        "SMILES_MAXLEN = 5\n",
        "smile = \"CON\"\n",
        "```\n",
        "#### **処理**\n",
        "```\n",
        "C → [1, 0, 0], O → [0, 1, 0], N → [0, 0, 1]\n",
        "```\n",
        "#### **出力**\n",
        "```\n",
        "[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  # (5 * 3 = 15次元)\n",
        "```\n",
        "このように、固定長 (`SMILES_MAXLEN`) のベクトルを作成する。\n",
        "\n",
        "---\n",
        "\n",
        "## **3. SMILESデータのベクトル化**\n",
        "```python\n",
        "X = np.array([smile2vec(vocab, SMILES_MAXLEN, smile) for smile in list(property_df[SMILES_COL])])\n",
        "```\n",
        "- `property_df[SMILES_COL]` の全てのSMILES文字列を `smile2vec()` を使って変換。\n",
        "- `np.array()` を使って、リストをNumPy配列に変換。\n",
        "\n",
        "---\n",
        "\n",
        "## **4. PyTorchテンソルに変換**\n",
        "```python\n",
        "X_tensor = torch.from_numpy(X).float()\n",
        "```\n",
        "- `numpy` の `X` を **PyTorchのテンソル** に変換。\n",
        "- `.float()` にすることで、ニューラルネットワークで処理しやすくする。\n",
        "\n",
        "---\n",
        "\n",
        "## **5. データセットとデータローダーの作成**\n",
        "```python\n",
        "dataset = TensorDataset(X_tensor)\n",
        "```\n",
        "- `X_tensor` を `TensorDataset` に変換し、PyTorchのデータセットとして扱えるようにする。\n",
        "\n",
        "```python\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "```\n",
        "- `DataLoader` を作成し、データをバッチ単位で処理できるようにする。\n",
        "- `batch_size=BATCH_SIZE`（例: 256）: 1回の学習で使うデータ数。\n",
        "- `shuffle=True`：データをランダムに並び替える。\n",
        "\n",
        "---\n",
        "\n",
        "## **まとめ**\n",
        "1. **SMILESをone-hotベクトル化**\n",
        "   - `smile2vec()` で `SMILES_MAXLEN × len(vocab)` のベクトルを作成。\n",
        "2. **データの変換**\n",
        "   - `NumPy配列` → `PyTorchテンソル` に変換。\n",
        "3. **データセット & データローダーの作成**\n",
        "   - `TensorDataset` を作成し、`DataLoader` でミニバッチ処理を可能にする。\n",
        "\n",
        "このコードは、GAN（または他の機械学習モデル）の学習に適した形式にSMILESデータを変換するための前処理を行っています！"
      ],
      "metadata": {
        "id": "DPPK79gk8hpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7s4GQq62rRF",
        "outputId": "52980935-452c-4152-d5cd-758f4eaad5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、Google Colab などの環境で **RDKit** という化学情報処理ライブラリをインストールするためのコマンドです。\n",
        "\n",
        "---\n",
        "\n",
        "## **1. コードの詳細**\n",
        "```python\n",
        "!pip install rdkit\n",
        "```\n",
        "- `!`（エクスクラメーションマーク）：Pythonスクリプト内でシェルコマンドを実行するための記号。\n",
        "- `pip install rdkit`：`rdkit` ライブラリを `pip`（Pythonのパッケージ管理システム）を使ってインストール。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. RDKitとは？**\n",
        "**RDKit（The RDKit: Open-Source Cheminformatics）** は、分子データを扱うための強力な **化学情報処理ライブラリ** です。  \n",
        "SMILES表記の分子を分子構造に変換したり、分子の特徴量を計算したりするのに使われます。\n",
        "\n",
        "### **主な機能**\n",
        "- **SMILES表記** の分子データの処理\n",
        "- **分子構造の可視化**\n",
        "- **物性予測**（分子量、LogP など）\n",
        "- **分子フィンガープリントの生成**（類似分子検索）\n",
        "- **ケモインフォマティクス**（分子データ解析）\n",
        "\n",
        "---\n",
        "\n",
        "## **3. インストール後の確認**\n",
        "インストールが成功したか確認するには、以下のコードを実行します。\n",
        "```python\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "\n",
        "print(rdkit.__version__)  # RDKitのバージョンを表示\n",
        "```\n",
        "問題なくバージョンが表示されれば、正常にインストールされています。\n",
        "\n",
        "---\n",
        "\n",
        "## **4. 注意点**\n",
        "- **Google Colab** では、`rdkit` はデフォルトではインストールされていないため、このコマンドが必要です。\n",
        "- **ローカル環境** では、Anaconda を使う場合は `conda install -c conda-forge rdkit` を推奨。\n",
        "\n",
        "---\n",
        "\n",
        "### **まとめ**\n",
        "- `!pip install rdkit` は、**RDKitライブラリをPython環境にインストール** するコマンド。\n",
        "- **RDKit** は、SMILES表記の分子データを処理・分析するための **化学情報処理ライブラリ**。\n",
        "- Google Colab では事前にインストールが必要。"
      ],
      "metadata": {
        "id": "zCvcixJ08rrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "def get_best_smile(out_tensor):\n",
        "    best_smile = \"\"\n",
        "    for vec in out_tensor:\n",
        "        vec = vec.reshape(SMILES_MAXLEN, len(vocab))\n",
        "        smile = \"\".join([vocab[torch.argmax(v).item()] for v in vec])\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "        while not mol:\n",
        "            if len(smile) == 0: break\n",
        "            smile = smile[:-1]\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "        if len(best_smile) < len(smile):\n",
        "            best_smile = smile\n",
        "\n",
        "    return best_smile"
      ],
      "metadata": {
        "id": "spClaXHS2lYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、**GAN などのモデルの出力テンソルを SMILES 文字列に変換し、最も良い（適切な）SMILES を選択する処理** を行っています。\n",
        "\n",
        "---\n",
        "\n",
        "## **1. 必要なライブラリのインポート**\n",
        "```python\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "```\n",
        "- `rdkit.Chem`:\n",
        "  - **RDKit の化学情報処理モジュール**。\n",
        "  - `Chem.MolFromSmiles(smile)` を使って、SMILES 文字列を分子オブジェクトに変換する。\n",
        "- `RDLogger.DisableLog('rdApp.*')`:\n",
        "  - RDKit のログ出力を抑制（警告メッセージを非表示にする）。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. `get_best_smile(out_tensor)` 関数の解説**\n",
        "```python\n",
        "def get_best_smile(out_tensor):\n",
        "    best_smile = \"\"  # 最も良いSMILESを保存する変数\n",
        "```\n",
        "- `out_tensor`: GAN などのモデルが出力したテンソル（複数の SMILES の候補が含まれる）。\n",
        "- `best_smile`: 最も良い SMILES 文字列を保存するための変数。\n",
        "\n",
        "---\n",
        "\n",
        "## **3. `out_tensor` の各データを処理**\n",
        "```python\n",
        "for vec in out_tensor:\n",
        "    vec = vec.reshape(SMILES_MAXLEN, len(vocab))  # 形状を変換 (SMILES_MAXLEN, vocab_size)\n",
        "    smile = \"\".join([vocab[torch.argmax(v).item()] for v in vec])  # One-hot を SMILES に変換\n",
        "```\n",
        "### **処理内容**\n",
        "1. `vec.reshape(SMILES_MAXLEN, len(vocab))`\n",
        "   - `out_tensor` から **1つのSMILESベクトルを取得し、2次元の形状 (SMILES_MAXLEN × vocab_size) に変換**。\n",
        "   - `vec.shape = (SMILES_MAXLEN × vocab_size)` のように 1次元になっている可能性があるので、元の形に戻す。\n",
        "\n",
        "2. **One-hot ベクトルを SMILES 文字列に変換**\n",
        "   ```python\n",
        "   smile = \"\".join([vocab[torch.argmax(v).item()] for v in vec])\n",
        "   ```\n",
        "   - `torch.argmax(v).item()` で **各文字の最大値のインデックス** を取得（one-hot エンコーディングから復元）。\n",
        "   - `vocab[index]` を取得し、リスト内包表記で SMILES 文字列を生成。\n",
        "   - `\"\".join([...])` で文字列として結合。\n",
        "\n",
        "---\n",
        "\n",
        "## **4. SMILES の修正**\n",
        "```python\n",
        "mol = Chem.MolFromSmiles(smile)  # RDKit を使って分子オブジェクトを作成\n",
        "while not mol:  # もしSMILESが無効なら修正する\n",
        "    if len(smile) == 0: break  # 空になったら終了\n",
        "    smile = smile[:-1]  # 末尾の1文字を削除\n",
        "    mol = Chem.MolFromSmiles(smile)  # 再度チェック\n",
        "```\n",
        "### **処理内容**\n",
        "- `Chem.MolFromSmiles(smile)` を使い、RDKit で **SMILES 文字列を分子オブジェクト (`mol`) に変換**。\n",
        "- もし `mol` が `None`（無効なSMILES）なら、**末尾を1文字ずつ削除** して有効なSMILESに修正。\n",
        "\n",
        "---\n",
        "\n",
        "## **5. 最も良いSMILESを選択**\n",
        "```python\n",
        "if len(best_smile) < len(smile):  # より長い有効なSMILESを採用\n",
        "    best_smile = smile\n",
        "```\n",
        "- **最も長い有効な SMILES を選択**。\n",
        "  - 途中で切られた短いSMILESではなく、できるだけ長い文字列を保持。\n",
        "\n",
        "---\n",
        "\n",
        "## **6. 最終的なSMILESを返す**\n",
        "```python\n",
        "return best_smile\n",
        "```\n",
        "- `best_smile`（最も適切なSMILES）を返す。\n",
        "\n",
        "---\n",
        "\n",
        "## **関数の動作例**\n",
        "### **入力**\n",
        "```python\n",
        "out_tensor = torch.tensor([\n",
        "    [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...],  # one-hotベクトル (flattened)\n",
        "])\n",
        "```\n",
        "### **処理**\n",
        "1. **One-hot ベクトル → 文字列**\n",
        "   ```\n",
        "   \"CCO(無効)O\" → 修正 → \"CCO\"\n",
        "   ```\n",
        "2. **最も長い有効なSMILESを選択**\n",
        "   - `best_smile = \"CCO\"`\n",
        "\n",
        "### **出力**\n",
        "```python\n",
        "\"CCO\"\n",
        "```\n",
        "---\n",
        "\n",
        "## **まとめ**\n",
        "- **GANの出力テンソルをSMILES文字列に変換** する関数。\n",
        "- **無効なSMILESを修正** し、最も長い有効なSMILESを選択。\n",
        "- **RDKit (`Chem.MolFromSmiles`) を使って分子として有効か確認** し、不適切なSMILESを修正。"
      ],
      "metadata": {
        "id": "EwHnwtZ1855O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_losses = []\n",
        "g_losses = []\n",
        "real_scores = []\n",
        "fake_scores = []\n",
        "\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data in enumerate(data_loader):\n",
        "\n",
        "        # 判別器の学習\n",
        "        outputs = D(data[0].to(device))\n",
        "        real_labels = torch.ones(outputs.shape[0], 1).to(device)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "        z = torch.randn(BATCH_SIZE, N_LATENT).to(device)\n",
        "        fake_data = G(z)\n",
        "        outputs = D(fake_data)\n",
        "        fake_labels = torch.zeros(outputs.shape[0], 1).to(device)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        if (i%2) == 0:\n",
        "            d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # 生成器の学習\n",
        "        z = torch.randn(BATCH_SIZE, N_LATENT).to(device)\n",
        "        fake_data = G(z)\n",
        "        outputs = D(fake_data)\n",
        "\n",
        "        real_labels = torch.ones(outputs.shape[0], 1).to(device)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # 結果の記録\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "        real_scores.append(real_score.mean().item())\n",
        "        fake_scores.append(fake_score.mean().item())\n",
        "        if (i+1) % 10 == 0 and (epoch+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.4f}, D(G(z)): {:.4f}'\n",
        "                  .format(epoch, NUM_EPOCHS, i+1, total_step, d_loss.item(), g_loss.item(),\n",
        "                          real_score.mean().item(), fake_score.mean().item()))\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Epoch[{}/{}], Generated SMILES: {}\".format(\n",
        "            epoch+1, NUM_EPOCHS, get_best_smile(fake_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H64Br-Od2qDo",
        "outputId": "bd08b7c4-1a49-4045-a543-4d36a016a3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/200], Step [10/35], d_loss: 1.3717, g_loss: 0.6960, D(x): 0.5060, D(G(z)): 0.4987\n",
            "Epoch [9/200], Step [20/35], d_loss: 1.3713, g_loss: 0.6966, D(x): 0.5060, D(G(z)): 0.4985\n",
            "Epoch [9/200], Step [30/35], d_loss: 1.3707, g_loss: 0.6966, D(x): 0.5061, D(G(z)): 0.4983\n",
            "Epoch[10/200], Generated SMILES: OC\n",
            "Epoch [19/200], Step [10/35], d_loss: 1.3535, g_loss: 0.7125, D(x): 0.5070, D(G(z)): 0.4905\n",
            "Epoch [19/200], Step [20/35], d_loss: 1.3521, g_loss: 0.7128, D(x): 0.5073, D(G(z)): 0.4900\n",
            "Epoch [19/200], Step [30/35], d_loss: 1.3518, g_loss: 0.7135, D(x): 0.5072, D(G(z)): 0.4898\n",
            "Epoch[20/200], Generated SMILES: C\\S\n",
            "Epoch [29/200], Step [10/35], d_loss: 1.3334, g_loss: 0.7305, D(x): 0.5084, D(G(z)): 0.4816\n",
            "Epoch [29/200], Step [20/35], d_loss: 1.3332, g_loss: 0.7314, D(x): 0.5084, D(G(z)): 0.4814\n",
            "Epoch [29/200], Step [30/35], d_loss: 1.3327, g_loss: 0.7317, D(x): 0.5085, D(G(z)): 0.4812\n",
            "Epoch[30/200], Generated SMILES: O\n",
            "Epoch [39/200], Step [10/35], d_loss: 1.3149, g_loss: 0.7468, D(x): 0.5100, D(G(z)): 0.4735\n",
            "Epoch [39/200], Step [20/35], d_loss: 1.3151, g_loss: 0.7475, D(x): 0.5100, D(G(z)): 0.4736\n",
            "Epoch [39/200], Step [30/35], d_loss: 1.3139, g_loss: 0.7478, D(x): 0.5099, D(G(z)): 0.4729\n",
            "Epoch[40/200], Generated SMILES: OCN\n",
            "Epoch [49/200], Step [10/35], d_loss: 1.2984, g_loss: 0.7626, D(x): 0.5115, D(G(z)): 0.4663\n",
            "Epoch [49/200], Step [20/35], d_loss: 1.2987, g_loss: 0.7634, D(x): 0.5115, D(G(z)): 0.4665\n",
            "Epoch [49/200], Step [30/35], d_loss: 1.2987, g_loss: 0.7633, D(x): 0.5112, D(G(z)): 0.4662\n",
            "Epoch[50/200], Generated SMILES: OC\n",
            "Epoch [59/200], Step [10/35], d_loss: 1.2836, g_loss: 0.7755, D(x): 0.5132, D(G(z)): 0.4601\n",
            "Epoch [59/200], Step [20/35], d_loss: 1.2844, g_loss: 0.7761, D(x): 0.5131, D(G(z)): 0.4605\n",
            "Epoch [59/200], Step [30/35], d_loss: 1.2829, g_loss: 0.7763, D(x): 0.5132, D(G(z)): 0.4598\n",
            "Epoch[60/200], Generated SMILES: O\n",
            "Epoch [69/200], Step [10/35], d_loss: 1.2715, g_loss: 0.7882, D(x): 0.5143, D(G(z)): 0.4547\n",
            "Epoch [69/200], Step [20/35], d_loss: 1.2703, g_loss: 0.7880, D(x): 0.5146, D(G(z)): 0.4544\n",
            "Epoch [69/200], Step [30/35], d_loss: 1.2701, g_loss: 0.7885, D(x): 0.5150, D(G(z)): 0.4547\n",
            "Epoch[70/200], Generated SMILES: OP\n",
            "Epoch [79/200], Step [10/35], d_loss: 1.2574, g_loss: 0.7996, D(x): 0.5165, D(G(z)): 0.4494\n",
            "Epoch [79/200], Step [20/35], d_loss: 1.2575, g_loss: 0.7997, D(x): 0.5164, D(G(z)): 0.4493\n",
            "Epoch [79/200], Step [30/35], d_loss: 1.2577, g_loss: 0.7996, D(x): 0.5162, D(G(z)): 0.4492\n",
            "Epoch[80/200], Generated SMILES: SN\n",
            "Epoch [89/200], Step [10/35], d_loss: 1.2460, g_loss: 0.8091, D(x): 0.5183, D(G(z)): 0.4450\n",
            "Epoch [89/200], Step [20/35], d_loss: 1.2450, g_loss: 0.8097, D(x): 0.5187, D(G(z)): 0.4448\n",
            "Epoch [89/200], Step [30/35], d_loss: 1.2461, g_loss: 0.8104, D(x): 0.5181, D(G(z)): 0.4448\n",
            "Epoch[90/200], Generated SMILES: CC\n",
            "Epoch [99/200], Step [10/35], d_loss: 1.2355, g_loss: 0.8184, D(x): 0.5203, D(G(z)): 0.4412\n",
            "Epoch [99/200], Step [20/35], d_loss: 1.2355, g_loss: 0.8177, D(x): 0.5202, D(G(z)): 0.4412\n",
            "Epoch [99/200], Step [30/35], d_loss: 1.2360, g_loss: 0.8184, D(x): 0.5202, D(G(z)): 0.4414\n",
            "Epoch[100/200], Generated SMILES: O\n",
            "Epoch [109/200], Step [10/35], d_loss: 1.2260, g_loss: 0.8251, D(x): 0.5220, D(G(z)): 0.4378\n",
            "Epoch [109/200], Step [20/35], d_loss: 1.2281, g_loss: 0.8252, D(x): 0.5215, D(G(z)): 0.4384\n",
            "Epoch [109/200], Step [30/35], d_loss: 1.2263, g_loss: 0.8254, D(x): 0.5223, D(G(z)): 0.4382\n",
            "Epoch[110/200], Generated SMILES: OC\n",
            "Epoch [119/200], Step [10/35], d_loss: 1.2189, g_loss: 0.8297, D(x): 0.5242, D(G(z)): 0.4362\n",
            "Epoch [119/200], Step [20/35], d_loss: 1.2186, g_loss: 0.8291, D(x): 0.5243, D(G(z)): 0.4361\n",
            "Epoch [119/200], Step [30/35], d_loss: 1.2191, g_loss: 0.8301, D(x): 0.5245, D(G(z)): 0.4365\n",
            "Epoch[120/200], Generated SMILES: CN\n",
            "Epoch [129/200], Step [10/35], d_loss: 1.2142, g_loss: 0.8311, D(x): 0.5260, D(G(z)): 0.4354\n",
            "Epoch [129/200], Step [20/35], d_loss: 1.2145, g_loss: 0.8330, D(x): 0.5259, D(G(z)): 0.4355\n",
            "Epoch [129/200], Step [30/35], d_loss: 1.2156, g_loss: 0.8325, D(x): 0.5253, D(G(z)): 0.4355\n",
            "Epoch[130/200], Generated SMILES: C\n",
            "Epoch [139/200], Step [10/35], d_loss: 1.2078, g_loss: 0.8323, D(x): 0.5287, D(G(z)): 0.4347\n",
            "Epoch [139/200], Step [20/35], d_loss: 1.2104, g_loss: 0.8319, D(x): 0.5279, D(G(z)): 0.4353\n",
            "Epoch [139/200], Step [30/35], d_loss: 1.2092, g_loss: 0.8323, D(x): 0.5282, D(G(z)): 0.4350\n",
            "Epoch[140/200], Generated SMILES: CCN\n",
            "Epoch [149/200], Step [10/35], d_loss: 1.2067, g_loss: 0.8307, D(x): 0.5302, D(G(z)): 0.4357\n",
            "Epoch [149/200], Step [20/35], d_loss: 1.2087, g_loss: 0.8301, D(x): 0.5298, D(G(z)): 0.4363\n",
            "Epoch [149/200], Step [30/35], d_loss: 1.2090, g_loss: 0.8306, D(x): 0.5300, D(G(z)): 0.4367\n",
            "Epoch[150/200], Generated SMILES: CC\n",
            "Epoch [159/200], Step [10/35], d_loss: 1.2046, g_loss: 0.8268, D(x): 0.5331, D(G(z)): 0.4374\n",
            "Epoch [159/200], Step [20/35], d_loss: 1.2046, g_loss: 0.8279, D(x): 0.5329, D(G(z)): 0.4373\n",
            "Epoch [159/200], Step [30/35], d_loss: 1.2052, g_loss: 0.8255, D(x): 0.5330, D(G(z)): 0.4377\n",
            "Epoch[160/200], Generated SMILES: ON\n",
            "Epoch [169/200], Step [10/35], d_loss: 1.2067, g_loss: 0.8218, D(x): 0.5346, D(G(z)): 0.4403\n",
            "Epoch [169/200], Step [20/35], d_loss: 1.2054, g_loss: 0.8219, D(x): 0.5348, D(G(z)): 0.4397\n",
            "Epoch [169/200], Step [30/35], d_loss: 1.2053, g_loss: 0.8218, D(x): 0.5348, D(G(z)): 0.4397\n",
            "Epoch[170/200], Generated SMILES: CN\n",
            "Epoch [179/200], Step [10/35], d_loss: 1.2066, g_loss: 0.8149, D(x): 0.5366, D(G(z)): 0.4423\n",
            "Epoch [179/200], Step [20/35], d_loss: 1.2059, g_loss: 0.8155, D(x): 0.5376, D(G(z)): 0.4429\n",
            "Epoch [179/200], Step [30/35], d_loss: 1.2061, g_loss: 0.8166, D(x): 0.5370, D(G(z)): 0.4424\n",
            "Epoch[180/200], Generated SMILES: CCN\n",
            "Epoch [189/200], Step [10/35], d_loss: 1.2077, g_loss: 0.8094, D(x): 0.5393, D(G(z)): 0.4457\n",
            "Epoch [189/200], Step [20/35], d_loss: 1.2072, g_loss: 0.8067, D(x): 0.5397, D(G(z)): 0.4458\n",
            "Epoch [189/200], Step [30/35], d_loss: 1.2095, g_loss: 0.8085, D(x): 0.5391, D(G(z)): 0.4465\n",
            "Epoch[190/200], Generated SMILES: CNN\n",
            "Epoch [199/200], Step [10/35], d_loss: 1.2152, g_loss: 0.8014, D(x): 0.5395, D(G(z)): 0.4500\n",
            "Epoch [199/200], Step [20/35], d_loss: 1.2116, g_loss: 0.8002, D(x): 0.5417, D(G(z)): 0.4502\n",
            "Epoch [199/200], Step [30/35], d_loss: 1.2104, g_loss: 0.7981, D(x): 0.5419, D(G(z)): 0.4498\n",
            "Epoch[200/200], Generated SMILES: CO\\NC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、**GAN（Generative Adversarial Network）を使って SMILES 文字列を生成するための学習ループ** です。  \n",
        "特に、**判別器 (Discriminator, D) と 生成器 (Generator, G) の学習** を行っています。\n",
        "\n",
        "---\n",
        "\n",
        "## **1. 変数の初期化**\n",
        "```python\n",
        "d_losses = []      # 判別器の損失を記録\n",
        "g_losses = []      # 生成器の損失を記録\n",
        "real_scores = []   # 本物データの判別結果\n",
        "fake_scores = []   # 偽物データの判別結果\n",
        "\n",
        "total_step = len(data_loader)  # 1エポックのステップ数（バッチ数）\n",
        "```\n",
        "- `d_losses`、`g_losses` は、それぞれ判別器・生成器の損失値を記録するリスト。\n",
        "- `real_scores`、`fake_scores` は、本物と偽物データの判別スコア（0～1）を記録。\n",
        "- `total_step` は、1エポックの総バッチ数（`data_loader` のバッチ数）。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. 学習ループ**\n",
        "```python\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data in enumerate(data_loader):\n",
        "```\n",
        "- `NUM_EPOCHS` 回のエポック（学習回数）を繰り返す。\n",
        "- `data_loader` からデータを取得し、`i` はバッチのインデックス。\n",
        "\n",
        "---\n",
        "\n",
        "## **3. 判別器 (D) の学習**\n",
        "```python\n",
        "# 本物データを D に入力\n",
        "outputs = D(data[0].to(device))  \n",
        "real_labels = torch.ones(outputs.shape[0], 1).to(device)  # 本物データのラベル (1)\n",
        "d_loss_real = criterion(outputs, real_labels)  # 本物データの損失\n",
        "real_score = outputs  # 本物データの判別結果\n",
        "```\n",
        "- **本物データを D に入力し、ラベル 1 との損失を計算**。\n",
        "\n",
        "```python\n",
        "# 偽物データを生成し D に入力\n",
        "z = torch.randn(BATCH_SIZE, N_LATENT).to(device)  # ランダムノイズを生成\n",
        "fake_data = G(z)  # 生成器 G で偽物データを生成\n",
        "outputs = D(fake_data)  # 偽物データを D に入力\n",
        "fake_labels = torch.zeros(outputs.shape[0], 1).to(device)  # 偽物データのラベル (0)\n",
        "d_loss_fake = criterion(outputs, fake_labels)  # 偽物データの損失\n",
        "fake_score = outputs  # 偽物データの判別結果\n",
        "```\n",
        "- **ノイズ `z` から生成器 `G` を通じて偽物データを生成**。\n",
        "- **D に入力し、偽物データが 0 に近いスコアになるよう学習**。\n",
        "\n",
        "```python\n",
        "d_loss = d_loss_real + d_loss_fake  # 本物・偽物の合計損失\n",
        "d_optimizer.zero_grad()\n",
        "g_optimizer.zero_grad()\n",
        "if (i % 2) == 0:  # 2回に1回だけ更新\n",
        "    d_loss.backward()\n",
        "d_optimizer.step()\n",
        "```\n",
        "- 判別器の損失を計算し、**2回に1回だけ D を更新**（判別器の学習が強すぎるのを防ぐ）。\n",
        "\n",
        "---\n",
        "\n",
        "## **4. 生成器 (G) の学習**\n",
        "```python\n",
        "z = torch.randn(BATCH_SIZE, N_LATENT).to(device)\n",
        "fake_data = G(z)\n",
        "outputs = D(fake_data)\n",
        "\n",
        "real_labels = torch.ones(outputs.shape[0], 1).to(device)  # 偽物データを「本物」と判定させる\n",
        "g_loss = criterion(outputs, real_labels)  # 生成器の損失 (D を騙せるか)\n",
        "```\n",
        "- 新たにノイズ `z` を生成し、`G` で偽物データを作る。\n",
        "- 偽物データを `D` に入力し、**本物 (1) だと判定させるように学習**。\n",
        "\n",
        "```python\n",
        "d_optimizer.zero_grad()\n",
        "g_optimizer.zero_grad()\n",
        "g_loss.backward()\n",
        "g_optimizer.step()\n",
        "```\n",
        "- **生成器 `G` のパラメータを更新**。\n",
        "\n",
        "---\n",
        "\n",
        "## **5. 損失・スコアの記録**\n",
        "```python\n",
        "d_losses.append(d_loss.item())\n",
        "g_losses.append(g_loss.item())\n",
        "real_scores.append(real_score.mean().item())\n",
        "fake_scores.append(fake_score.mean().item())\n",
        "```\n",
        "- 判別器・生成器の損失、D の出力スコア（本物・偽物）をリストに保存。\n",
        "\n",
        "---\n",
        "\n",
        "## **6. ログの出力**\n",
        "```python\n",
        "if (i+1) % 10 == 0 and (epoch+1) % 10 == 0:\n",
        "    print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.4f}, D(G(z)): {:.4f}'\n",
        "          .format(epoch, NUM_EPOCHS, i+1, total_step, d_loss.item(), g_loss.item(),\n",
        "                  real_score.mean().item(), fake_score.mean().item()))\n",
        "```\n",
        "- **10ステップごと & 10エポックごとに学習状況を表示**。\n",
        "- `D(x)`: 本物データの平均スコア（1 に近いほど良い）。\n",
        "- `D(G(z))`: 偽物データの平均スコア（0 に近いほど良い）。\n",
        "\n",
        "---\n",
        "\n",
        "## **7. SMILES 文字列の生成**\n",
        "```python\n",
        "if (epoch+1) % 10 == 0:\n",
        "    print(\"Epoch[{}/{}], Generated SMILES: {}\".format(\n",
        "        epoch+1, NUM_EPOCHS, get_best_smile(fake_data)))\n",
        "```\n",
        "- **10エポックごとに、生成した SMILES を `get_best_smile` で変換して表示**。\n",
        "\n",
        "---\n",
        "\n",
        "## **まとめ**\n",
        "- **判別器 (D) の学習**\n",
        "  - 本物データ (`data[0]`) を入力し、1 に近づける。\n",
        "  - 偽物データ (`G(z)`) を入力し、0 に近づける。\n",
        "  - `D` を 2回に1回だけ更新（学習バランス調整）。\n",
        "\n",
        "- **生成器 (G) の学習**\n",
        "  - `D(G(z))` の出力を **1 に近づけるように学習**（D を騙せるようにする）。\n",
        "\n",
        "- **結果の記録**\n",
        "  - 損失 (`d_loss, g_loss`)、スコア (`D(x), D(G(z))`) を記録。\n",
        "  - 10エポックごとに **生成した SMILES を出力**。"
      ],
      "metadata": {
        "id": "-xGl_wNnrOdf"
      }
    }
  ]
}